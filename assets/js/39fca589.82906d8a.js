"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[7845],{5680:(e,n,t)=>{t.d(n,{xA:()=>u,yg:()=>d});var r=t(6540);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function c(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,r,o=function(e,n){if(null==e)return{};var t,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var i=r.createContext({}),l=function(e){var n=r.useContext(i),t=n;return e&&(t="function"==typeof e?e(n):c(c({},n),e)),t},u=function(e){var n=l(e.components);return r.createElement(i.Provider,{value:n},e.children)},p="mdxType",g={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},m=r.forwardRef((function(e,n){var t=e.components,o=e.mdxType,a=e.originalType,i=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),p=l(t),m=o,d=p["".concat(i,".").concat(m)]||p[m]||g[m]||a;return t?r.createElement(d,c(c({ref:n},u),{},{components:t})):r.createElement(d,c({ref:n},u))}));function d(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var a=t.length,c=new Array(a);c[0]=m;var s={};for(var i in n)hasOwnProperty.call(n,i)&&(s[i]=n[i]);s.originalType=e,s[p]="string"==typeof e?e:o,c[1]=s;for(var l=2;l<a;l++)c[l]=t[l];return r.createElement.apply(null,c)}return r.createElement.apply(null,t)}m.displayName="MDXCreateElement"},8838:(e,n,t)=>{t.r(n),t.d(n,{contentTitle:()=>c,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>i});var r=t(8168),o=(t(6540),t(5680));const a={},c="Data Ingestion",s={unversionedId:"gpt-researcher/context/data-ingestion",id:"gpt-researcher/context/data-ingestion",isDocsHomePage:!1,title:"Data Ingestion",description:"When you're dealing with a large amount of context data, you may want to start meditating upon a standalone process for data ingestion.",source:"@site/docs/gpt-researcher/context/data-ingestion.md",sourceDirName:"gpt-researcher/context",slug:"/gpt-researcher/context/data-ingestion",permalink:"/docs/gpt-researcher/context/data-ingestion",editUrl:"https://github.com/assafelovic/gpt-researcher/tree/master/docs/docs/gpt-researcher/context/data-ingestion.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"Vector Stores",permalink:"/docs/gpt-researcher/context/vector-stores"},next:{title:"Configure LLM",permalink:"/docs/gpt-researcher/llms/llms"}},i=[{value:"Step 1: Transform your content into Langchain Documents",id:"step-1-transform-your-content-into-langchain-documents",children:[],level:2},{value:"Step 2: Insert your Langchain Documents into a Langchain VectorStore",id:"step-2-insert-your-langchain-documents-into-a-langchain-vectorstore",children:[],level:2},{value:"Step 3: Pass your Langchain Vectorstore into your GPTR report",id:"step-3-pass-your-langchain-vectorstore-into-your-gptr-report",children:[],level:2}],l={toc:i},u="wrapper";function p(e){let{components:n,...t}=e;return(0,o.yg)(u,(0,r.A)({},l,t,{components:n,mdxType:"MDXLayout"}),(0,o.yg)("h1",{id:"data-ingestion"},"Data Ingestion"),(0,o.yg)("p",null,"When you're dealing with a large amount of context data, you may want to start meditating upon a standalone process for data ingestion."),(0,o.yg)("p",null,"Some signs that the system is telling you to move to a custom data ingestion process:"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"Your embedding model is hitting API rate limits"),(0,o.yg)("li",{parentName:"ul"},"Your Langchain VectorStore's underlying database needs rate limiting"),(0,o.yg)("li",{parentName:"ul"},"You sense you need to add custom pacing/throttling logic in your Python code")),(0,o.yg)("p",null,"As mentioned in our ",(0,o.yg)("a",{parentName:"p",href:"https://www.youtube.com/watch?v=yRuduRCblbg"},"YouTube Tutorial Series"),", GPTR is using ",(0,o.yg)("a",{parentName:"p",href:"https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html"},"Langchain Documents")," and ",(0,o.yg)("a",{parentName:"p",href:"https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/"},"Langchain VectorStores")," under the hood."),(0,o.yg)("p",null,"These are 2 beautiful abstractions that make the GPTR architecture highly configurable."),(0,o.yg)("p",null,"The current research flow, whether you're generating reports on web or local documents, is:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"Step 1: transform your content (web results or local documents) into Langchain Documents\n")),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"Step 2: Insert your Langchain Documents into a Langchain VectorStore\n")),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"Step 3: Pass your Langchain Vectorstore into your GPTR report ([more on that here](https://docs.gptr.dev/docs/gpt-researcher/context/vector-stores) and below)\n")),(0,o.yg)("p",null,"Code samples below:"),(0,o.yg)("p",null,"Assuming your .env variables are like so:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"OPENAI_API_KEY={Your OpenAI API Key here}\nTAVILY_API_KEY={Your Tavily API Key here}\nPGVECTOR_CONNECTION_STRING=postgresql://username:password...\n")),(0,o.yg)("p",null,"Below is a custom data ingestion process that you can use to ingest your data into a Langchain VectorStore. See a ",(0,o.yg)("a",{parentName:"p",href:"https://github.com/assafelovic/gpt-researcher/pull/819#issue-2501632831"},"full working example here"),".\nIn this example, we're using a Postgres VectorStore to embed data of a Github Branch, but you can use ",(0,o.yg)("a",{parentName:"p",href:"https://python.langchain.com/v0.2/docs/integrations/vectorstores/"},"any supported Langchain VectorStore"),".hasattr"),(0,o.yg)("p",null,"Note that when you create the Langchain Documents, you should include as metadata the ",(0,o.yg)("inlineCode",{parentName:"p"},"source")," and ",(0,o.yg)("inlineCode",{parentName:"p"},"title")," fields in order for GPTR to leverage your Documents seamlessly. In the example below, we're splitting the documents list into chunks of 100 & then inserting 1 chunk at a time into the vector store."),(0,o.yg)("h2",{id:"step-1-transform-your-content-into-langchain-documents"},"Step 1: Transform your content into Langchain Documents"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'from langchain_core.documents import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nasync def transform_to_langchain_docs(self, directory_structure):\n    documents = []\n    splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=30)\n    run_timestamp = datetime.utcnow().strftime(\'%Y%m%d%H%M%S\')\n\n    for file_name in directory_structure:\n        if not file_name.endswith(\'/\'):\n            try:\n                content = self.repo.get_contents(file_name, ref=self.branch_name)\n                try:\n                    decoded_content = base64.b64decode(content.content).decode()\n                except Exception as e:\n                    print(f"Error decoding content: {e}")\n                    print("the problematic file_name is", file_name)\n                    continue\n                print("file_name", file_name)\n                print("content", decoded_content)\n\n                # Split each document into smaller chunks\n                chunks = splitter.split_text(decoded_content)\n\n                # Extract metadata for each chunk\n                for index, chunk in enumerate(chunks):\n                    metadata = {\n                        "id": f"{run_timestamp}_{uuid4()}",  # Generate a unique UUID for each document\n                        "source": file_name,\n                        "title": file_name,\n                        "extension": os.path.splitext(file_name)[1],\n                        "file_path": file_name\n                    }\n                    document = Document(\n                        page_content=chunk,\n                        metadata=metadata\n                    )\n                    documents.append(document)\n\n            except Exception as e:\n                print(f"Error saving to vector store: {e}")\n                return None\n\n    await save_to_vector_store(documents)\n')),(0,o.yg)("h2",{id:"step-2-insert-your-langchain-documents-into-a-langchain-vectorstore"},"Step 2: Insert your Langchain Documents into a Langchain VectorStore"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'from langchain_postgres import PGVector\nfrom langchain_postgres.vectorstores import PGVector\nfrom sqlalchemy.ext.asyncio import create_async_engine\n\nfrom langchain_community.embeddings import OpenAIEmbeddings\n\nasync def save_to_vector_store(self, documents):\n    # The documents are already Document objects, so we don\'t need to convert them\n    embeddings = OpenAIEmbeddings()\n    # self.vector_store = FAISS.from_documents(documents, embeddings)\n    pgvector_connection_string = os.environ["PGVECTOR_CONNECTION_STRING"]\n\n    collection_name = "my_docs"\n\n    vector_store = PGVector(\n        embeddings=embeddings,\n        collection_name=collection_name,\n        connection=pgvector_connection_string,\n        use_jsonb=True\n    )\n\n    # for faiss\n    # self.vector_store = vector_store.add_documents(documents, ids=[doc.metadata["id"] for doc in documents])\n\n    # Split the documents list into chunks of 100\n    for i in range(0, len(documents), 100):\n        chunk = documents[i:i+100]\n        # Insert the chunk into the vector store\n        vector_store.add_documents(chunk, ids=[doc.metadata["id"] for doc in chunk])\n')),(0,o.yg)("h2",{id:"step-3-pass-your-langchain-vectorstore-into-your-gptr-report"},"Step 3: Pass your Langchain Vectorstore into your GPTR report"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'async_connection_string = pgvector_connection_string.replace("postgresql://", "postgresql+psycopg://")\n\n# Initialize the async engine with the psycopg3 driver\nasync_engine = create_async_engine(\n    async_connection_string,\n    echo=True\n)\n\nasync_vector_store = PGVector(\n    embeddings=embeddings,\n    collection_name=collection_name,\n    connection=async_engine,\n    use_jsonb=True\n)\n\n\nresearcher = GPTResearcher(\n    query=query,\n    report_type="research_report",\n    report_source="langchain_vectorstore",\n    vector_store=async_vector_store,\n)\nawait researcher.conduct_research()\nreport = await researcher.write_report()\n')))}p.isMDXComponent=!0}}]);