"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[2337],{7032:(e,r,s)=>{s.r(r),s.d(r,{assets:()=>l,contentTitle:()=>c,default:()=>h,frontMatter:()=>t,metadata:()=>n,toc:()=>a});const n=JSON.parse('{"id":"gpt-researcher/gptr/config","title":"Configuration","description":"The config.py enables you to customize GPT Researcher to your specific needs and preferences.","source":"@site/docs/gpt-researcher/gptr/config.md","sourceDirName":"gpt-researcher/gptr","slug":"/gpt-researcher/gptr/config","permalink":"/docs/gpt-researcher/gptr/config","draft":false,"unlisted":false,"editUrl":"https://github.com/assafelovic/gpt-researcher/tree/master/docs/docs/gpt-researcher/gptr/config.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"AI-Assisted Development","permalink":"/docs/gpt-researcher/gptr/ai-development"},"next":{"title":"Scraping Options","permalink":"/docs/gpt-researcher/gptr/scraping"}}');var o=s(4848),i=s(8453);const t={},c="Configuration",l={},a=[{value:"Deep Research Configuration",id:"deep-research-configuration",level:2}];function d(e){const r={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(r.header,{children:(0,o.jsx)(r.h1,{id:"configuration",children:"Configuration"})}),"\n",(0,o.jsx)(r.p,{children:"The config.py enables you to customize GPT Researcher to your specific needs and preferences."}),"\n",(0,o.jsx)(r.p,{children:"Thanks to our amazing community and contributions, GPT Researcher supports multiple LLMs and Retrievers.\nIn addition, GPT Researcher can be tailored to various report formats (such as APA), word count, research iterations depth, etc."}),"\n",(0,o.jsxs)(r.p,{children:["GPT Researcher defaults to our recommended suite of integrations: ",(0,o.jsx)(r.a,{href:"https://platform.openai.com/docs/overview",children:"OpenAI"})," for LLM calls and ",(0,o.jsx)(r.a,{href:"https://app.tavily.com",children:"Tavily API"})," for retrieving real-time web information."]}),"\n",(0,o.jsx)(r.p,{children:"As seen below, OpenAI still stands as the superior LLM. We assume it will stay this way for some time, and that prices will only continue to decrease, while performance and speed increase over time."}),"\n",(0,o.jsx)("div",{style:{marginBottom:"10px"},children:(0,o.jsx)("img",{align:"center",height:"350",src:"/img/leaderboard.png"})}),"\n",(0,o.jsxs)(r.p,{children:["The default config.py file can be found in ",(0,o.jsx)(r.code,{children:"/gpt_researcher/config/"}),". It supports various options for customizing GPT Researcher to your needs.\nYou can also include your own external JSON file ",(0,o.jsx)(r.code,{children:"config.json"})," by adding the path in the ",(0,o.jsx)(r.code,{children:"config_path"})," param.\nThe config JSON should follow the format/keys in the default config. Below is a sample config.json file to help get you started:"]}),"\n",(0,o.jsx)(r.pre,{children:(0,o.jsx)(r.code,{className:"language-json",children:'{\n  "RETRIEVER": "tavily",\n  "EMBEDDING": "openai:text-embedding-3-small",\n  "SIMILARITY_THRESHOLD": 0.42,\n  "FAST_LLM": "openai:gpt-4o-mini",\n  "SMART_LLM": "openai:gpt-4.1",\n  "STRATEGIC_LLM": "openai:o4-mini",\n  "LANGUAGE": "english",\n  "CURATE_SOURCES": false,\n  "FAST_TOKEN_LIMIT": 2000,\n  "SMART_TOKEN_LIMIT": 4000,\n  "STRATEGIC_TOKEN_LIMIT": 4000,\n  "BROWSE_CHUNK_MAX_LENGTH": 8192,\n  "SUMMARY_TOKEN_LIMIT": 700,\n  "TEMPERATURE": 0.4,\n  "DOC_PATH": "./my-docs",\n  "REPORT_SOURCE": "web"\n}\n'})}),"\n",(0,o.jsx)(r.p,{children:"For example, to start GPT-Researcher and specify a specific config you would do this:"}),"\n",(0,o.jsx)(r.pre,{children:(0,o.jsx)(r.code,{className:"language-bash",children:"python gpt_researcher/main.py --config_path my_config.json\n"})}),"\n",(0,o.jsxs)(r.p,{children:[(0,o.jsx)(r.strong,{children:"Please follow the config.py file for additional future support"}),"."]}),"\n",(0,o.jsx)(r.p,{children:"Below is a list of current supported options:"}),"\n",(0,o.jsxs)(r.ul,{children:["\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"RETRIEVER"})}),": Web search engine used for retrieving sources. Defaults to ",(0,o.jsx)(r.code,{children:"tavily"}),". Options: ",(0,o.jsx)(r.code,{children:"duckduckgo"}),", ",(0,o.jsx)(r.code,{children:"bing"}),", ",(0,o.jsx)(r.code,{children:"google"}),", ",(0,o.jsx)(r.code,{children:"searchapi"}),", ",(0,o.jsx)(r.code,{children:"serper"}),", ",(0,o.jsx)(r.code,{children:"searx"}),". ",(0,o.jsx)(r.a,{href:"https://github.com/assafelovic/gpt-researcher/tree/master/gpt_researcher/retrievers",children:"Check here"})," for supported retrievers"]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"EMBEDDING"})}),": Embedding model. Defaults to ",(0,o.jsx)(r.code,{children:"openai:text-embedding-3-small"}),". Options: ",(0,o.jsx)(r.code,{children:"ollama"}),", ",(0,o.jsx)(r.code,{children:"huggingface"}),", ",(0,o.jsx)(r.code,{children:"azure_openai"}),", ",(0,o.jsx)(r.code,{children:"custom"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"SIMILARITY_THRESHOLD"})}),": Threshold value for similarity comparison when processing documents. Defaults to ",(0,o.jsx)(r.code,{children:"0.42"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"FAST_LLM"})}),": Model name for fast LLM operations such summaries. Defaults to ",(0,o.jsx)(r.code,{children:"openai:gpt-4o-mini"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"SMART_LLM"})}),": Model name for smart operations like generating research reports and reasoning. Defaults to ",(0,o.jsx)(r.code,{children:"openai:gpt-5"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"STRATEGIC_LLM"})}),": Model name for strategic operations like generating research plans and strategies. Defaults to ",(0,o.jsx)(r.code,{children:"openai:gpt-5-mini"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"LANGUAGE"})}),": Language to be used for the final research report. Defaults to ",(0,o.jsx)(r.code,{children:"english"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"CURATE_SOURCES"})}),": Whether to curate sources for research. This step adds an LLM run which may increase costs and total run time but improves quality of source selection. Defaults to ",(0,o.jsx)(r.code,{children:"False"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"FAST_TOKEN_LIMIT"})}),": Maximum token limit for fast LLM responses. Defaults to ",(0,o.jsx)(r.code,{children:"2000"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"SMART_TOKEN_LIMIT"})}),": Maximum token limit for smart LLM responses. Defaults to ",(0,o.jsx)(r.code,{children:"4000"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"STRATEGIC_TOKEN_LIMIT"})}),": Maximum token limit for strategic LLM responses. Defaults to ",(0,o.jsx)(r.code,{children:"4000"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"BROWSE_CHUNK_MAX_LENGTH"})}),": Maximum length of text chunks to browse in web sources. Defaults to ",(0,o.jsx)(r.code,{children:"8192"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"SUMMARY_TOKEN_LIMIT"})}),": Maximum token limit for generating summaries. Defaults to ",(0,o.jsx)(r.code,{children:"700"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"TEMPERATURE"})}),": Sampling temperature for LLM responses, typically between 0 and 1. A higher value results in more randomness and creativity, while a lower value results in more focused and deterministic responses. Defaults to ",(0,o.jsx)(r.code,{children:"0.4"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"USER_AGENT"})}),": Custom User-Agent string for web crawling and web requests."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"MAX_SEARCH_RESULTS_PER_QUERY"})}),": Maximum number of search results to retrieve per query. Defaults to ",(0,o.jsx)(r.code,{children:"5"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"MEMORY_BACKEND"})}),": Backend used for memory operations, such as local storage of temporary data. Defaults to ",(0,o.jsx)(r.code,{children:"local"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"TOTAL_WORDS"})}),": Total word count limit for document generation or processing tasks. Defaults to ",(0,o.jsx)(r.code,{children:"1200"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"REPORT_FORMAT"})}),": Preferred format for report generation. Defaults to ",(0,o.jsx)(r.code,{children:"APA"}),". Consider formats like ",(0,o.jsx)(r.code,{children:"MLA"}),", ",(0,o.jsx)(r.code,{children:"CMS"}),", ",(0,o.jsx)(r.code,{children:"Harvard style"}),", ",(0,o.jsx)(r.code,{children:"IEEE"}),", etc."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"MAX_ITERATIONS"})}),": Maximum number of iterations for processes like query expansion or search refinement. Defaults to ",(0,o.jsx)(r.code,{children:"3"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"AGENT_ROLE"})}),": Role of the agent. This configures the behavior of specialized research agents. Defaults to ",(0,o.jsx)(r.code,{children:"None"}),". When set, it activates role-specific prompting and techniques tailored to particular research domains."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"MAX_SUBTOPICS"})}),": Maximum number of subtopics to generate or consider. Defaults to ",(0,o.jsx)(r.code,{children:"3"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"SCRAPER"})}),": Web scraper to use for gathering information. Defaults to ",(0,o.jsx)(r.code,{children:"bs"})," (BeautifulSoup). You can also use ",(0,o.jsx)(r.a,{href:"https://github.com/codelucas/newspaper",children:"newspaper"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"MAX_SCRAPER_WORKERS"})}),": Maximum number of concurrent scraper workers per research. Defaults to ",(0,o.jsx)(r.code,{children:"15"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"REPORT_SOURCE"})}),": Source for the research report data. Defaults to ",(0,o.jsx)(r.code,{children:"web"})," for online research. Can be set to ",(0,o.jsx)(r.code,{children:"doc"})," for local document-based research. This determines where GPT Researcher gathers its primary information from."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"DOC_PATH"})}),": Path to read and research local documents. Defaults to ",(0,o.jsx)(r.code,{children:"./my-docs"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"PROMPT_FAMILY"})}),": The family of prompts and prompt formatting to use. Defaults to prompting optimized for GPT models. See the full list of options in ",(0,o.jsx)(r.a,{href:"https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/utils/enum.py#L56",children:"enum.py"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"LLM_KWARGS"})}),": Json formatted dict of additional keyword args to be passed to the LLM provider class when instantiating it. This is primarily useful for clients like Ollama that allow for additional keyword arguments such as ",(0,o.jsx)(r.code,{children:"num_ctx"})," that influence the inference calls."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"EMBEDDING_KWARGS"})}),": Json formatted dict of additional keyword args to be passed to the embedding provider class when instantiating it."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"DEEP_RESEARCH_BREADTH"})}),": Controls the breadth of deep research, defining how many parallel paths to explore. Defaults to ",(0,o.jsx)(r.code,{children:"3"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"DEEP_RESEARCH_DEPTH"})}),": Controls the depth of deep research, defining how many sequential searches to perform. Defaults to ",(0,o.jsx)(r.code,{children:"2"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"DEEP_RESEARCH_CONCURRENCY"})}),": Controls the concurrency level for deep research operations. Defaults to ",(0,o.jsx)(r.code,{children:"4"}),"."]}),"\n",(0,o.jsxs)(r.li,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"REASONING_EFFORT"})}),": Controls the reasoning effort of strategic models. Default to ",(0,o.jsx)(r.code,{children:"medium"}),"."]}),"\n"]}),"\n",(0,o.jsx)(r.h2,{id:"deep-research-configuration",children:"Deep Research Configuration"}),"\n",(0,o.jsx)(r.p,{children:"The deep research parameters allow you to fine-tune how GPT Researcher explores complex topics that require extensive knowledge gathering. These parameters work together to determine the thoroughness and efficiency of the research process:"}),"\n",(0,o.jsxs)(r.ul,{children:["\n",(0,o.jsxs)(r.li,{children:["\n",(0,o.jsxs)(r.p,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"DEEP_RESEARCH_BREADTH"})}),": Controls how many parallel research paths are explored simultaneously. A higher value (e.g., 5) causes the researcher to investigate more diverse subtopics at each step, resulting in broader coverage but potentially less focus on core themes. The default value of ",(0,o.jsx)(r.code,{children:"3"})," provides a balanced approach between breadth and depth."]}),"\n"]}),"\n",(0,o.jsxs)(r.li,{children:["\n",(0,o.jsxs)(r.p,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"DEEP_RESEARCH_DEPTH"})}),": Determines how many sequential search iterations GPT Researcher performs for each research path. A higher value (e.g., 3-4) allows for following citation trails and diving deeper into specialized information, but increases research time substantially. The default value of ",(0,o.jsx)(r.code,{children:"2"})," ensures reasonable depth while maintaining practical completion times."]}),"\n"]}),"\n",(0,o.jsxs)(r.li,{children:["\n",(0,o.jsxs)(r.p,{children:[(0,o.jsx)(r.strong,{children:(0,o.jsx)(r.code,{children:"DEEP_RESEARCH_CONCURRENCY"})}),": Sets how many concurrent operations can run during deep research. Higher values speed up the research process on capable systems but may increase API rate limit issues or resource consumption. The default value of ",(0,o.jsx)(r.code,{children:"4"})," is suitable for most environments, but can be increased on systems with more resources or decreased if you experience performance issues."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(r.p,{children:"For academic or highly specialized research, consider increasing both breadth and depth (e.g., BREADTH=4, DEPTH=3). For quick exploratory research, lower values (e.g., BREADTH=2, DEPTH=1) will provide faster results with less detail."}),"\n",(0,o.jsxs)(r.p,{children:["To change the default configurations, you can simply add env variables to your ",(0,o.jsx)(r.code,{children:".env"})," file as named above or export manually in your local project directory."]}),"\n",(0,o.jsx)(r.p,{children:"For example, to manually change the search engine and report format:"}),"\n",(0,o.jsx)(r.pre,{children:(0,o.jsx)(r.code,{className:"language-bash",children:"export RETRIEVER=bing\nexport REPORT_FORMAT=IEEE\n"})}),"\n",(0,o.jsxs)(r.p,{children:["Please note that you might need to export additional env vars and obtain API keys for other supported search retrievers and LLM providers. Please follow your console logs for further assistance.\nTo learn more about additional LLM support you can check out the docs ",(0,o.jsx)(r.a,{href:"/docs/gpt-researcher/llms/llms",children:"here"}),"."]})]})}function h(e={}){const{wrapper:r}={...(0,i.R)(),...e.components};return r?(0,o.jsx)(r,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,r,s)=>{s.d(r,{R:()=>t,x:()=>c});var n=s(6540);const o={},i=n.createContext(o);function t(e){const r=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function c(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:t(e.components),n.createElement(i.Provider,{value:r},e.children)}}}]);